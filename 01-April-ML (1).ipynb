{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630ab26b-d2a5-470a-ba77-763ac02bb8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinear Regression is used to handle regression problems whereas Logistic regression is used to handle the classification problems.\\nLinear regression provides a continuous output but Logistic regression provides discreet output.\\n\\nLogistic regression is used to calculate the probability of a binary event occurring, and to deal with issues of classification. \\nFor example, predicting if an incoming email is spam or not spam, or predicting\\nif a credit card transaction is fraudulent or not fraudulent.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "\"\"\"\n",
    "Linear Regression is used to handle regression problems whereas Logistic regression is used to handle the classification problems.\n",
    "Linear regression provides a continuous output but Logistic regression provides discreet output.\n",
    "\n",
    "Logistic regression is used to calculate the probability of a binary event occurring, and to deal with issues of classification. \n",
    "For example, predicting if an incoming email is spam or not spam, or predicting\n",
    "if a credit card transaction is fraudulent or not fraudulent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e4cde6-46fe-46b9-bd2f-3ade344253c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe cost function used in Logistic Regression is Log Loss .\\nCost(hθ(x),y)=−ylog(hθ(x))−(1−y)log(1−hθ(x)\\nA Cost function basically compares the predicted values with the actual values.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2 :\n",
    "\"\"\"\n",
    "The cost function used in Logistic Regression is Log Loss .\n",
    "Cost(hθ(x),y)=−ylog(hθ(x))−(1−y)log(1−hθ(x)\n",
    "A Cost function basically compares the predicted values with the actual values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1529ebc6-6e7d-4877-85a3-1a1cb619a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization is used to reduce the complexity of the prediction function by imposing a penalty.\\nIn order to avoid overfitting, it is necessary to use additional techniques\\n(e.g. cross-validation, regularization, early stopping, pruning, or Bayesian priors).\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "'''\n",
    "Regularization is used to reduce the complexity of the prediction function by imposing a penalty.\n",
    "In order to avoid overfitting, it is necessary to use additional techniques\n",
    "(e.g. cross-validation, regularization, early stopping, pruning, or Bayesian priors).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "106921cd-d3b1-460c-93e7-812b67c85bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe ROC curve is produced by calculating and plotting the true positive rate against the false positive rate\\nfor a single classifier at a variety of thresholds. \\nFor example,\\nin logistic regression,the threshold would be the predicted probability of an observation belonging to the positive class.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "'''\n",
    "The ROC curve is produced by calculating and plotting the true positive rate against the false positive rate\n",
    "for a single classifier at a variety of thresholds. \n",
    "For example,\n",
    "in logistic regression,the threshold would be the predicted probability of an observation belonging to the positive class.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "308942f4-c69e-4820-bcb1-414f2aded960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLogit Model\\nThis is the Logistic regression-based model which selects the features based on the p-value score of the feature. \\nThe features with p-value less than 0.05 are considered to be the more relevant feature.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "'''\n",
    "Logit Model\n",
    "This is the Logistic regression-based model which selects the features based on the p-value score of the feature. \n",
    "The features with p-value less than 0.05 are considered to be the more relevant feature.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a278e11e-67ae-4555-b0c1-6f10b8ef6cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n7 Techniques to Handle Imbalanced Data. \\nUse the right evaluation metrics. \\nResample the training set. \\nUse K-fold Cross-Validation in the Right Way. \\nEnsemble Different Resampled Datasets. \\nResample with Different Ratios. \\nCluster the abundant class. \\nDesign Your Model\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 6 :\n",
    "\"\"\"\n",
    "7 Techniques to Handle Imbalanced Data. \n",
    "Use the right evaluation metrics. \n",
    "Resample the training set. \n",
    "Use K-fold Cross-Validation in the Right Way. \n",
    "Ensemble Different Resampled Datasets. \n",
    "Resample with Different Ratios. \n",
    "Cluster the abundant class. \n",
    "Design Your Model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee3ad24-f06c-40a0-87df-6d613eda063a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLogistic regression may not be accurate if the sample size is too small. \\nIf the sample size is on the small side, the model produced by logistic regression is based on a smaller number of actual observations.\\nThis can result in overfitting\\nsteps to reduce multicollinearity :\\n1.The severity of the problems increases with the degree of the multicollinearity. \\nTherefore, if you have only moderate multicollinearity, you may not need to resolve it.\\n2.Multicollinearity affects only the specific independent variables that are correlated. \\nTherefore, if multicollinearity is not present for the independent variables that you are particularly interested in,\\nyou may not need to resolve it. Suppose your model contains the experimental variables of interest and some control variables.\\nIf high multicollinearity exists for the control variables but not the experimental variables, then you can interpret the \\nexperimental variables without problems.\\n3.Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, \\nprecision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, \\nand you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "'''\n",
    "Logistic regression may not be accurate if the sample size is too small. \n",
    "If the sample size is on the small side, the model produced by logistic regression is based on a smaller number of actual observations.\n",
    "This can result in overfitting\n",
    "steps to reduce multicollinearity :\n",
    "1.The severity of the problems increases with the degree of the multicollinearity. \n",
    "Therefore, if you have only moderate multicollinearity, you may not need to resolve it.\n",
    "2.Multicollinearity affects only the specific independent variables that are correlated. \n",
    "Therefore, if multicollinearity is not present for the independent variables that you are particularly interested in,\n",
    "you may not need to resolve it. Suppose your model contains the experimental variables of interest and some control variables.\n",
    "If high multicollinearity exists for the control variables but not the experimental variables, then you can interpret the \n",
    "experimental variables without problems.\n",
    "3.Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, \n",
    "precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, \n",
    "and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
